{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from utils.hard_rules import hard_rules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('Data/_20230622-130921_training.tsv', sep='\\t', header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns = {0: 'Value',\n",
    "                     1: 'Evidence_type',\n",
    "                     2: 'CCV'}, \n",
    "          inplace = True)\n",
    "\n",
    "# Remove ENG instances\n",
    "ENG_ccvs = [ccv for ccv in df['CCV'].unique() if 'ENG' in ccv]\n",
    "df = df[ ~df['CCV'].isin(ENG_ccvs) ]\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop CCVs in which no prediction can be made\n",
    "dropped_CCVs = ['CCV:00004', 'CCV:00025', 'CCV:00028', 'CCV:00029', 'CCV:00035', 'CCV:00036', 'CCV:00042', 'CCV:00043',\n",
    "    'CCV:00047', 'CCV:00048', 'CCV:00049', 'CCV:00072', 'CCV:00084', 'CCV:00087', 'CCV:00088',\n",
    "    'CCV:00079', 'CCV:00082', 'CCV:00092', 'CCV:00093', 'CCV:00096']\n",
    "df = df[ ~df['CCV'].isin( dropped_CCVs )]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard rule predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12446035/12446035 [03:28<00:00, 59569.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df['Predictions'] = df[['Value','Evidence_type']].progress_apply(lambda x: hard_rules(x.Value, x.Evidence_type), axis=1)  \n",
    "\n",
    "# Keep hard-rule predictions\n",
    "df = df[ df['Predictions'].astype('str') != \"None\" ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.729%\n",
      "Precision: 0.957\n",
      "Recall: 0.956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   974,      0,      0,      0,      0,      0,      0,      0],\n",
       "       [     0,    867,      0,      0,      0,      0,      0,      0],\n",
       "       [     0,      0, 167991,  24846,      0,      0,      0,      0],\n",
       "       [     0,      0,  40180, 140840,      0,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,    144,      0,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,     14,      0,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,    408,      0],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,    236]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('Data/data.npz', allow_pickle=True)['X']\n",
    "y = np.load('Data/data.npz', allow_pickle=True)['y']\n",
    "\n",
    "# Load Label encoder\n",
    "encoder = pickle.load(open('Model/Label_encoder.pkl', 'rb'))\n",
    "# Load prediction model\n",
    "model = joblib.load(\"Model/model.joblib\")\n",
    "# Get predictions\n",
    "pred = encoder.inverse_transform( model.predict(X) )\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "Accuracy = 100*metrics.accuracy_score(y, pred)\n",
    "Recall = metrics.recall_score(y, pred, average='macro')\n",
    "Precision = metrics.precision_score(y, pred, average='macro')   \n",
    "CM = metrics.confusion_matrix(y, pred)\n",
    "\n",
    "print(f'Accuracy: {Accuracy:.3f}%')\n",
    "print(f'Precision: {Precision:.3f}')\n",
    "print(f'Recall: {Recall:.3f}')\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CCV:00002       1.00      1.00      1.00       974\n",
      "   CCV:00003       1.00      1.00      1.00       867\n",
      "   CCV:00005       0.81      0.87      0.84    192837\n",
      "   CCV:00008       0.85      0.78      0.81    181020\n",
      "   CCV:00011       1.00      1.00      1.00       144\n",
      "   CCV:00030       1.00      1.00      1.00        14\n",
      "   CCV:00052       1.00      1.00      1.00       408\n",
      "   CCV:00065       1.00      1.00      1.00       236\n",
      "\n",
      "    accuracy                           0.83    376500\n",
      "   macro avg       0.96      0.96      0.96    376500\n",
      "weighted avg       0.83      0.83      0.83    376500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( metrics.classification_report(y_true=y, y_pred=pred, target_names=np.unique(y)) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = np.concatenate([df['Predictions'].values, pred])\n",
    "Actuals = np.concatenate([df['CCV'].values, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.404%\n",
      "Precision: 0.992\n",
      "Recall: 0.991\n"
     ]
    }
   ],
   "source": [
    "Accuracy = 100*metrics.accuracy_score(Actuals, Predictions)\n",
    "Recall = metrics.recall_score(Actuals, Predictions, average='macro')\n",
    "Precision = metrics.precision_score(Actuals, Predictions, average='macro')   \n",
    "\n",
    "print(f'Accuracy: {Accuracy:.3f}%')\n",
    "print(f'Precision: {Precision:.3f}')\n",
    "print(f'Recall: {Recall:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   974,      0,      0, ...,      0,      0,      0],\n",
       "       [     0,    867,      0, ...,      0,      0,      0],\n",
       "       [     0,      0, 167991, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [     0,      0,      0, ..., 440209,      0,      0],\n",
       "       [     0,      0,      0, ...,      0, 440209,      0],\n",
       "       [     0,      0,      0, ...,      0,      0,    999]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = metrics.confusion_matrix(Actuals, Predictions)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pretty_confusion_matrix import pp_matrix_from_data\n",
    "pp_matrix_from_data(CM=CM, \n",
    "                    cmap=\"Oranges\", \n",
    "                    figsize=(25, 25))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
